---
title: "Decision Tree"
author: "Claire Yoon (ndq7xj)"
date: "2023-01-04"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(plyr)
library(plotly)
library(randomForest)
library(rio)
library(caret)
library(ROCR)
library(tidyverse)
library(rpart)
library(psych)
library(pROC)
library(rpart.plot)
library(rattle)
library(tidyverse)
library(class)
library(plotly)
library(MLmetrics)
library(mltools)
library(data.table)
library(RColorBrewer)
```


#### Read in the data
```{r}
#water_quality <- read.csv("/Users/stephaniefissel/Library/Mobile Documents/com~apple~CloudDocs/ds final project/water_potability.csv")
water_quality <- read.csv("~/Desktop/R/water-quality-project/water_potability.csv")
```

### Clean the data
There were 1/3 missing values in Sulfate variable, but we didnâ€™t delete it because sulfate showed one of the crucial factors for predicting non-potability because it leads to diarrhea and some problems. After handling with the missing values, we have 2011 objects.
```{r}
water_quality <- water_quality[complete.cases(water_quality), ]
#dim(water_quality)
```

### Rename and factorize the 'Potability' variable
We renamed the categorical values (1 and 0) to "potable" and "non_potable" </br>
```{r}
water_quality$Potability <- as.factor(water_quality$Potability)
water_quality$Potability <- fct_collapse(water_quality$Potability, 
                           potable = "1",
                           non_potable = "0")
table(water_quality$Potability)
str(water_quality)

glimpse(water_quality)
```

### Splitting the data with tuning and test & Creating Decision Trees
#### First Decision Tree Model
```{r}
#We do not need to check for correlated variables because correlation does not impact decision trees.
#Decision trees make greedy, localized decisions that are not dependent on previous steps or other variables in the tree model.  
set.seed(777)
partition <- caret::createDataPartition(water_quality$Potability,
                                           times=1,
                                           p = 0.80,
                                           groups=1,
                                           list=FALSE)
train <- water_quality[partition, ]
tune_and_test <- water_quality[-partition, ]
#train

tune_and_test_index <- createDataPartition(tune_and_test$Potability,
                                           p = .5,
                                           list = FALSE,
                                           times = 1)

tune <- tune_and_test[tune_and_test_index, ]
test <- tune_and_test[-tune_and_test_index, ]

#dim(train)
#dim(test) 
#dim(tune)

# Build the model
# Choose the features and classes, slightly different approach for caret, need to create features and target sets from the training data.
#str(water_quality)
features <- train[,-10] # dropping the target variable
target <- train$Potability
#str(features)

#str(target)
fitControl <- trainControl(method = "repeatedcv",
                          number = 10,
                          repeats = 5,
                          classProbs = TRUE,
                          summaryFunction = twoClassSummary) 

# Train the model without setting tree grid
set.seed(777)
water_mdl1 <- train(x=features,
                y=target,
                method="rpart2",
                trControl=fitControl,
                metric="ROC")
plot(water_mdl1)
varImp(water_mdl1)
rpart.plot(water_mdl1$finalModel, type=1,extra=101)
water_mdl1 # maxdepth is 11

```


#### Second Decision Tree Model
```{r}
# Train the model
tree.grid <- expand.grid(maxdepth=c(1:6))
set.seed(777)
water_mdl2 <- train(x=features,
                y=target,
                method="rpart2",
                trControl=fitControl,
                tuneGrid=tree.grid,
                metric="ROC")

plot(water_mdl2)
varImp(water_mdl2)

rpart.plot(water_mdl2$finalModel, type=1,extra=101)

water_mdl2 # maxdepth is 6
```


### Confusion Matrix, Statistics, and Density Plot on Two Decision Trees
#### First Model

```{r}
predictandCM<- function(model,data,modeltype,ref)
{
  pred <-predict(model,data,type=modeltype)
  confusionMatrix(pred, reference=ref, positive = 'potable')
}

predictandCM(water_mdl1, tune, "raw", tune$Potability)
water_pred_tune = predict(water_mdl1,tune,tune$Potability, type= "prob")
water_pred_tune_labels = predict(water_mdl1,tune,tune$Potability,type = "raw")
plot(density(water_pred_tune$potable))

```
TN FN
FP TP
#### Second Model
```{r}
predictandCM(water_mdl2, tune, "raw", tune$Potability) 
water_pred_tune2 = predict(water_mdl2,tune,tune$Potability, type= "prob")
water_pred_tune_labels2 = predict(water_mdl2,tune,tune$Potability,type = "raw")
plot(density(water_pred_tune2$potable))
#View(as_tibble(water_pred_tune_labels))
```


### Understanding Confusion Matrix
True Positive(TP): Potable water correctly identified as potable </br>
- Predicted condition: potable </br>
- Actual condition: potable </br>
</br>
False Positive(FP): Non potable water incorrectly identified as potable </br>
- Predicted condition: non-potable </br>
- Actual condition: potable </br>
</br>
False Negative(FN): Potable water incorrectly identified as non-potable </br>
- Predicted condition: non-potable </br>
- Actual condition: potable </br>
<br>
True Negative(TN): Non potable water correctly identified as non-potable </br>
- Predicted condition: non-potable </br>
- Actual condition: non-potable </br>
</br>

Considering the real world situation, False Negative does not cause detrimental effects because people do not drink water if it is identified as non-potable even though it is actually potable. However, the opposite case (False Positive) will cause significant problems (people will be sick), so we need to make sure to reduce the number of False Positive and the model should be reliable.

### Sensitivity & Specificity
Sensitivity = number of true positives / (number of true positives + number of false negatives) </br>
= number of correctly identified as potable water / total number of potable water

Specificity = number of true negatives / (total number of true negatives + number of false positives) </br>
= number of correctly identified as non-potable water / total number of non-potable water
</br>
Our goal is to reduce the number of False Positive and get higher Specificity

```{r}
# Use the the confusion matrix function on your predictions to check a variety of metrics and comment on the metric that might be best for this type of analysis given your question.  

water_eval <- caret::confusionMatrix(water_pred_tune_labels2, 
                as.factor(tune$Potability), 
                dnn=c("Prediction", "Actual"),
                positive="potable",
                mode = "everything")
#water_eval
```

### Adjusting Threshold with the Second Decision Tree Model
We were adjusting threshold between 0.2 and 0.8, then found 0.6 is a good point considering the balance of sensitivity and specificity. However, the accuracy of the model is still low, so we need to improve on our model and reduce the False Negative values.
```{r}
# With the percentages you generated in step 10,select several different threshold levels using the threshold function we created and interpret the results. What patterns do you notice, did the evaluation metrics change?

adjust_thres <- function(x, y, z) {
  #x=pred_probablities, y=threshold, z=tune_outcome
  thres <- as.factor(ifelse(x > y, "potable","non_potable"))
  confusionMatrix(thres, z, positive = "potable", dnn=c("Prediction", "Actual"), mode = "everything")
}

adjust_thres(water_pred_tune2$potable,y=.6,tune$Potability)
```


### Improve on the initial models
```{r}
# load the data
water_quality <- read.csv("~/Desktop/R/water-quality-project/water_potability.csv")
# data cleaning
water_quality <- water_quality[complete.cases(water_quality), ]
# factorize
water_quality$Potability <- as.factor(water_quality$Potability)
water_quality$Potability <- fct_collapse(water_quality$Potability, 
                           potable = "1",
                           non_potable = "0")

```



```{r}
set.seed(777)
partition <- caret::createDataPartition(water_quality$Potability,
                                           times=1,
                                           p = 0.80,
                                           groups=1,
                                           list=FALSE)
train <- water_quality[partition, ]
tune_and_test <- water_quality[-partition, ]
#train

tune_and_test_index <- createDataPartition(tune_and_test$Potability,
                                           p = .5,
                                           list = FALSE,
                                           times = 1)

tune <- tune_and_test[tune_and_test_index, ]
test <- tune_and_test[-tune_and_test_index, ]

#dim(train)
#dim(test) 
#dim(tune)

# Build the model
# Choose the features and classes, slightly different approach for caret, need to create features and target sets from the training data.
features <- train[,-10] # dropping the target variable
target <- train$Potability

fitControl <- trainControl(method = "repeatedcv",
                          number = 10,
                          repeats = 5,
                          returnResamp = "all",
                          classProbs = TRUE,
                          allowParallel = TRUE,
                          summaryFunction = twoClassSummary) 



grid <- expand.grid(.winnow = c(TRUE,FALSE), # try with reducing the feature space and without
                    .trials=c(1,5,10,15,20), # number of boosting iterations to try
                    .model="tree")  # use a decision tree model

set.seed(777)  # set seed for reproducibility
new_mdl <- train(x=features,  # train the model with the features to predict the target, salary
                y=target,
                method="C5.0",  # use C5.0 model that works by splitting the tree based on maximum info gain
                tuneGrid=grid,
                trControl=fitControl,
                verbose=TRUE)

new_mdl
varImp(new_mdl)
plot(new_mdl)
predictandCM(new_mdl, tune, "raw", tune$Potability)
water_pred_tune = predict(new_mdl,tune,tune$Potability, type= "prob")
water_pred_tune_labels = predict(new_mdl,tune,tune$Potability,type = "raw")
#plot(density(water_pred_tune$non_potable))
```


```{r}
# visualize the re-sample distributions with different winnows and trials
xyplot(new_mdl,type = c("g", "p", "smooth"))
```

```{r}
# estimate the target variable.
set.seed(777)  # set seed 
pred_tune = predict(new_mdl,tune)  # predict using the model on the tune set

pred_tune_prob <- predict(new_mdl,tune, tune, type = "prob")  # save the raw probabilities for each observation being in the positive or negative class

probs_and_results <- cbind(pred_tune_prob, data.frame(`actual_potability`= tune$Potability))

head(probs_and_results)

```

### Predicted vs. Actual Target Matrix
```{r}
table(as.factor(pred_tune), as.factor(tune$Potability))
```


### Confusion Matrix with Model on Tune Set
```{r}
# Use the the confusion matrix function to check a variety of metrics 
# and comment on the metric that might be best for this type of analysis given your question.  
set.seed(777) 
eval <- confusionMatrix(as.factor(pred_tune),   # confusion matrix on the predictions and actual values 
                as.factor(tune$Potability),
                positive='potable',  # assign the positive class 
                dnn=c("Prediction", "Actual"), 
                mode = "everything")  # output all evaluation metrics 


eval
```

### ROC and AUC 

```{r}
# Generate a ROC and AUC output, interpret the results

# Put predictions and targets in one table 
pred_tune_tibble <- tibble(pred_class=pred_tune, pred_prob=pred_tune_prob$`potable`, target=as.numeric(tune$Potability))
#pred_tune_tibble2 <- tibble(pred_class=pred_tune, pred_prob=pred_tune_prob$`non_potable`, target=as.numeric(tune$Potability))

pred <- prediction(pred_tune_tibble$pred_prob, pred_tune_tibble$target) # use predicted prob and target at different threshold levels to build ROC curve
#pred2 <- prediction(pred_tune_tibble2$pred_prob, pred_tune_tibble$target)
ROC_perf <- performance(pred,"tpr","fpr") # prediction for True Positive Rate and False Positive Rate 
# TPR = TP / (TP + FN)
# FPR = FP / (FP + TN)
#ROC_perf2 <- performance(pred,"tnr","fnr") # prediction for True Negative Rate and False Positive Rate 

plot(ROC_perf, colorize=TRUE) # plot ROC curve
#plot(ROC_perf2, colorize=TRUE)

```


```{r}
tree_perf_AUC <- performance(pred,"auc")  # calculate auc on the predictions

tree_perf_AUC@y.values

#tree_perf_AUC <- performance(pred2,"auc")  # calculate auc on the predictions

#tree_perf_AUC@y.values
```

### Evaluate Model with Different Hyper-Parameters

```{r}
fitControl2 <- trainControl(method = "LGOCV",  # Use leave group out cross validation with 5 folds
                          number = 5,
                          returnResamp="all",
                          classProbs = TRUE,
                          allowParallel = TRUE) 

grid2 <- expand.grid(.winnow = c(TRUE,FALSE), # try with reducing the feature space and without
                    .trials=c(1,5,10,15,20,25,30,40),   # number of boosting iterations to try - add greater values
                    .model="tree") # use a decision tree model


set.seed(777)  # set seed for reproducibility
new_mdl2 <- train(x=features,  # train the model again with the new hyper-parameters
                y=target,
                method="C5.0",
                tuneGrid=grid2,
                trControl=fitControl2,
                verbose=TRUE)


set.seed(777) # set seed for reproducibility
pred_tune2 = predict(new_mdl2,tune, type= "prob")$`potable` # predict with the new model on the tune data
set.seed(1)

thres_tune2 <- as.factor(ifelse(pred_tune2 > 0.4, "potable","non_potable"))  # compare the predictions to the 0.4 threshold

eval2 <- confusionMatrix(as.factor(thres_tune2),   # evaluate the new predictions from the new model
                as.factor(tune$Potability),
                positive='potable',
                dnn=c("Prediction", "Actual"), 
                mode = "everything")  # output all of the evaluation metric 

eval2
```

### Test another model

```{r}
set.seed(1)
pred_test = predict(new_mdl2, test, type= "prob")$`potable`  # predict with the second model on the test set 

thres_test <- as.factor(ifelse(pred_test > 0.4, "potable","non_potable"))  # compare the predictions to the 0.4 threshold

set.seed(1)
confusionMatrix(as.factor(thres_test),  # Use the confusion matrix to evaluate how the model performed on the test set 
                as.factor(test$Potability), 
                dnn=c("Prediction", "Actual"), 
                positive='potable',
                mode = "everything")
```




