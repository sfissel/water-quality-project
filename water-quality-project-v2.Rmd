---
title: "Water Quality Presentation2"
author: "Group 3: Stephanie Fissel, Beza Gashe, Claire Yoon"
date: "January 4, 2023"
output: 
  html_document:
    theme: lumen
    code_folding: hide
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    toc_depth: 3
editor_options: 
chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(plyr)
library(plotly)
library(randomForest)
library(rio)
library(caret)
library(ROCR)
library(tidyverse)
library(rpart)
library(psych)
library(pROC)
library(rpart.plot)
library(rattle)
library(tidyverse)
library(class)
library(plotly)
library(MLmetrics)
library(mltools)
library(data.table)
library(RColorBrewer)
```


#### Read in the data
```{r}
#water_quality <- read.csv("/Users/stephaniefissel/Library/Mobile Documents/com~apple~CloudDocs/ds final project/water_potability.csv")
water_quality <- read.csv("~/Desktop/R/water-quality-project/water_potability.csv")

#head(water_quality)
#dim(water_quality)
```

### Clean the data
There were 1/3 missing values in Sulfate variable, but we didnâ€™t delete it because sulfate showed one of the crucial factors for predicting non-potability because it leads to diarrhea and some problems. After handling with the missing values, we have 2011 objects.
```{r}
water_quality <- water_quality[complete.cases(water_quality), ]
#dim(water_quality)
```

### Rename and factorize the 'Potability' variable
We renamed the categorical values (1 and 0) to "potable" and "non_potable" </br>
We are focusing on non potability of drinking water, so our positive value is "non_potable"
```{r}
water_quality$Potability <- as.factor(water_quality$Potability)
water_quality$Potability <- fct_collapse(water_quality$Potability, 
                           potable = "1",
                           non_potable = "0")
```

### Splitting the data with tuning and test & Creating Decision Trees
#### First Decision Tree Model
```{r}
#We do not need to check for correlated variables because correlation does not impact decision trees.
#Decision trees make greedy, localized decisions that are not dependent on previous steps or other variables in the tree model.  
set.seed(777)
partition <- caret::createDataPartition(water_quality$Potability,
                                           times=1,
                                           p = 0.80,
                                           groups=1,
                                           list=FALSE)
train <- water_quality[partition, ]
tune_and_test <- water_quality[-partition, ]
#train

tune_and_test_index <- createDataPartition(tune_and_test$Potability,
                                           p = .5,
                                           list = FALSE,
                                           times = 1)

tune <- tune_and_test[tune_and_test_index, ]
test <- tune_and_test[-tune_and_test_index, ]

#dim(train)
#dim(test) 
#dim(tune)

# Build the model
# Choose the features and classes, slightly different approach for caret, need to create features and target sets from the training data.
#str(water_quality)
features <- train[,-10] # dropping the target variable
target <- train$Potability
#str(features)

#str(target)
fitControl <- trainControl(method = "repeatedcv",
                          number = 10,
                          repeats = 5,
                          classProbs = TRUE,
                          summaryFunction = twoClassSummary) 

tree.grid <- expand.grid(maxdepth=c(1:6))

# Train the model without setting tree grid
set.seed(777)
water_mdl1 <- train(x=features,
                y=target,
                method="rpart2",
                trControl=fitControl,
                metric="ROC")
plot(water_mdl1)
varImp(water_mdl1)
rpart.plot(water_mdl1$finalModel, type=1,extra=101)
water_mdl1 # maxdepth is 11

```


#### Second Decision Tree Model
```{r}
# Train the model
set.seed(777)
water_mdl2 <- train(x=features,
                y=target,
                method="rpart2",
                trControl=fitControl,
                tuneGrid=tree.grid,
                metric="ROC")

plot(water_mdl2)
varImp(water_mdl2)

rpart.plot(water_mdl2$finalModel, type=1,extra=101)

water_mdl2 # maxdepth is 6
```


### Confusion Matrix, Statistics, and Density Plot on Two Decision Trees
#### First Model

```{r}
predictandCM<- function(model,data,modeltype,ref)
{
  pred <-predict(model,data,type=modeltype)
  confusionMatrix(pred, reference=ref, positive = 'non_potable')
}

predictandCM(water_mdl1, tune, "raw", tune$Potability)
water_pred_tune = predict(water_mdl1,tune,tune$Potability, type= "prob")
water_pred_tune_labels = predict(water_mdl1,tune,tune$Potability,type = "raw")
plot(density(water_pred_tune$non_potable))

```

#### Second Model
```{r}
predictandCM(water_mdl2, tune, "raw", tune$Potability) 
water_pred_tune2 = predict(water_mdl2,tune,tune$Potability, type= "prob")
water_pred_tune_labels2 = predict(water_mdl2,tune,tune$Potability,type = "raw")
plot(density(water_pred_tune2$non_potable))
#View(as_tibble(water_pred_tune_labels))
```


### Understanding Confusion Matrix
True Positive(TP) </br>
- Predicted condition: non-potable </br>
- Actual condition: non-potable </br>
</br>
False Positive(FP) </br>
- Predicted condition: non-potable </br>
- Actual condition: potable </br>
</br>
False Negative(FN) </br>
- Predicted condition: potable </br>
- Actual condition: non-potable </br>
</br>
True Negative(TN) </br>
- Predicted condition: potable </br>
- Actual condition: potable </br>
<br>
Considering the real world, False Positive doesn't really make any problems because people do not drink water if it is predicted as non-potable even though it is potable. However, the opposite case (False Negative) will cause significant problems (people will be sick), so we need to make sure to reduce False Negative portion and the model should be reliable.

```{r}
# Use the the confusion matrix function on your predictions to check a variety of metrics and comment on the metric that might be best for this type of analysis given your question.  

water_eval <- caret::confusionMatrix(water_pred_tune_labels2, 
                as.factor(tune$Potability), 
                dnn=c("Prediction", "Actual"),
                positive="non_potable",
                mode = "everything")
#water_eval
```

### Adjusting Threshold with the Second Decision Tree Model
We were adjusting threshold between 0.2 and 0.8, then found 0.6 is a good point considering the balance of sensitivity and specificity. However, the accuracy of the model is still low, so we need to improve on our model and reduce the False Negative values.
```{r}
# With the percentages you generated in step 10,select several different threshold levels using the threshold function we created and interpret the results. What patterns do you notice, did the evaluation metrics change?

adjust_thres <- function(x, y, z) {
  #x=pred_probablities, y=threshold, z=tune_outcome
  thres <- as.factor(ifelse(x > y, "non_potable","potable"))
  confusionMatrix(thres, z, positive = "non_potable", dnn=c("Prediction", "Actual"), mode = "everything")
}

adjust_thres(water_pred_tune2$non_potable,y=.6,tune$Potability)
```
